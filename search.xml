<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>「BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding」总结</title>
      <link href="/2019/05/29/%E3%80%8CBERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding%E3%80%8D%E6%80%BB%E7%BB%93/"/>
      <url>/2019/05/29/%E3%80%8CBERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding%E3%80%8D%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p><img src="//blog.xixilili.cn/2019/05/29/「BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding」总结/4-1.png" alt="image"></p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>bert model是多层双向transformer编码器。详细可参考<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a>。</p><p>这里需要了解bert模型的相关参数：</p><ul><li>L: number of layer, e.g. transformer block</li><li>H: hiddensize</li><li>A: self-attention head number</li></ul><p>bert模型有两个大小：</p><ul><li>base：L=12，H=768，A=12，total parameters=340M</li><li>large：L=24，H=1024，A=16，total parameters=110M</li></ul><h2 id="Input-Presentation"><a href="#Input-Presentation" class="headerlink" title="Input Presentation"></a>Input Presentation</h2><p><img src="//blog.xixilili.cn/2019/05/29/「BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding」总结/4-2.png" alt="image"></p><p>输入是形式sentence pair，e.g. [question, answer]的pair，[A, B]。输入的表示由三部分构成（求和）：</p><ul><li>segment embedding：句子embedding，因为前面提到训练数据都是由两个句子构成的，那么 <strong><em>每个句子有个句子整体的embedding项对应给每个单词</em></strong></li><li>token embedding：单词embedding,这个就是我们之前一直提到的 <strong><em>单词embedding</em></strong></li><li>position embedding：位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对 <strong><em>位置信息进行编码</em></strong></li></ul><p>有几个注意事项：</p><ul><li>word pieces：数据中的有些单词会被分割为word piece，playing这类词在数据中会被表示为play和##ing，其中##是word piece的分割标记。</li><li>sequence length：sentence pair的总长度小于等于512 tokens</li><li>特殊标志：[CLS]表示句子的开始, [SEP]表示句子的分割</li><li>在预训练的时候，若使用单个句子则仅使用[A, B]中的A</li></ul><h2 id="Pre-training-Task"><a href="#Pre-training-Task" class="headerlink" title="Pre-training Task"></a>Pre-training Task</h2><h3 id="Masked-LM"><a href="#Masked-LM" class="headerlink" title="Masked LM"></a>Masked LM</h3><p>任务简述：将句子中的一些词mask掉，然后预测mask的词语是什么。</p><p>在这个任务中，对mask进行了特殊处理：</p><ul><li>对一个sequence中15%的word piece token进行随机mask</li><li>使用[mask]替换原来的word，但并非所有的都用[mask]替换，替换规则是：<ul><li>80%的mask使用[mask]标志进行替换</li><li>10%的mask用随机的其他token替代</li><li>10%的mask保持使用原本的token</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input: the man went to the [MASK1] . he bought a [MASK2] of milk.</span><br><span class="line">Labels: [MASK1] = store; [MASK2] = gallon</span><br></pre></td></tr></table></figure><p>这样做的意义是：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。</p><h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><p>任务简述：对于[A, B]预测sentence B是不是sentence A的下一个句子，如果是label是 IsNextSentence， 如果不是标注为 NotNextSentence，负例的构造方式是，保留A然后在预料中随机找另一个句子作为B，正负例的比例是1:1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Sentence A: the man went to the store .</span><br><span class="line">Sentence B: he bought a gallon of milk .</span><br><span class="line">Label: IsNextSentence</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Sentence A: the man went to the store .</span><br><span class="line">Sentence B: penguins are flightless .</span><br><span class="line">Label: NotNextSentence</span><br></pre></td></tr></table></figure><p>这样做的意义是：我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。</p><h2 id="Pre-training-Procedure"><a href="#Pre-training-Procedure" class="headerlink" title="Pre-training Procedure"></a>Pre-training Procedure</h2><p>介绍了dataset，generate input sequence，task的运行，参数的设置等</p><h2 id="Fine-tuning-Procedure"><a href="#Fine-tuning-Procedure" class="headerlink" title="Fine-tuning Procedure"></a>Fine-tuning Procedure</h2><p>介绍了对分类问题的fine-tuning</p><h1 id="查缺补漏"><a href="#查缺补漏" class="headerlink" title="查缺补漏"></a>查缺补漏</h1><h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>Position Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。</p><p>Position Embedding的公式如下所示，这里的意思是将为p的位置映射为一个 d_pos 维的位置向量，这个向量的第i个元素的数值就是 PE_i(p)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PE_&#123;2i&#125;(p) = sin(p/ 10000^&#123;&#123;2i&#125; / &#123;d_&#123;pos&#125;&#125;&#125; )</span><br><span class="line"></span><br><span class="line">PE_&#123;2i+1&#125;(p)=cos(p/10000^&#123;&#123;2i&#125;/d_&#123;pos&#125;&#125;)</span><br></pre></td></tr></table></figure><p>Position Embedding本身是一个绝对位置的信息，但在语言中，相对位置也很重要，Google选择前述的位置向量公式的一个重要原因是：由于我们有sin(α+β)=sinαcosβ+cosαsinβ以及cos(α+β)=cosαcosβ−sinαsinβ，这表明位置p+k的向量可以表示成位置p的向量的线性变换，这提供了表达相对位置信息的可能性。</p><p>结合位置向量和词向量有几个可选方案，可以把它们拼接起来作为一个新向量，也可以把位置向量定义为跟词向量一样大小，然后两者加起来。</p><p>参考资料：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li><li><a href="https://github.com/google-research/bert" target="_blank" rel="noopener">google-research/bert</a></li><li><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href="https://kexue.fm/archives/4765" target="_blank" rel="noopener">《Attention is All You Need》浅读（简介+代码）</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bert </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自然语言处理中的预训练</title>
      <link href="/2019/05/24/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83-%E6%80%BB%E7%BB%93/"/>
      <url>/2019/05/24/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83-%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>“预训练”的思路来源于图像领域，因此，首先追本溯源，从图像角度出发了解其基本思路。</p><h1 id="图像领域的预训练"><a href="#图像领域的预训练" class="headerlink" title="图像领域的预训练"></a>图像领域的预训练</h1><h2 id="什么是预训练"><a href="#什么是预训练" class="headerlink" title="什么是预训练"></a>什么是预训练</h2><p>以简要的预训练步骤进行说明：</p><ol><li>设计好网络结构</li><li>在任务A上训练模型、学习参数</li><li>保存模型参数，以后备用（即与训练模型）</li></ol><h2 id="如何使用与训练模型"><a href="#如何使用与训练模型" class="headerlink" title="如何使用与训练模型"></a>如何使用与训练模型</h2><p>使用与训练模型一般有两种做法：</p><ol><li>Frozen：浅层加载的参数在新的任务上不改变。（一个简单的例子，加载预训练词向量，在模型训练中，词向量参数不改变）</li><li>Fine-Tuning：把参数进行微调，使得更适应当前任务。（同样以词向量为例，加载预训练词向量，词向量不固定，可随模型的训练改变）</li></ol><h2 id="使用预训练的优势"><a href="#使用预训练的优势" class="headerlink" title="使用预训练的优势"></a>使用预训练的优势</h2><p><strong><em>问题：</em></strong> 新任务训练数据少，则不能很好的训练复杂网络。</p><p><strong><em>解决方案：</em></strong> 此时可使用大的数据集预训练模型，然后根据新任务的数据进行Fine-Tuning，使其能够解决新任务，如：ImageNet的使用。</p><h2 id="为什么预训练的思路可行"><a href="#为什么预训练的思路可行" class="headerlink" title="为什么预训练的思路可行"></a>为什么预训练的思路可行</h2><p>预训练好的网络参数，底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性。因此，一般用底层预训练好的参数初始化新任务网络参数。与之相反，高层特征跟任务关联较大。</p><p>在图像领域，越是底层的特征越是不论什么领域的图像都会具备的，比如：边角线弧线等底层基础特征。在高层则可学习到任务相关的特征，如：五官的轮廓、脸轮廓等特征。</p><h1 id="Word-Embedding"><a href="#Word-Embedding" class="headerlink" title="Word Embedding"></a>Word Embedding</h1><p>Word Embedding是从语言模型一步步演化形成的，下面魔术Word Embedding是如何形成的。</p><h2 id="语言模型（LM）"><a href="#语言模型（LM）" class="headerlink" title="语言模型（LM）"></a>语言模型（LM）</h2><p>评估哪个句子更像合理的句子，根据句子中词语的上下文信息，计算条件概率进行评估。</p><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/1.png" alt="image"></p><h2 id="神经网络语言模型（NNLM）"><a href="#神经网络语言模型（NNLM）" class="headerlink" title="神经网络语言模型（NNLM）"></a>神经网络语言模型（NNLM）</h2><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/2.png" alt="image"></p><p>学习任务是输入某个句中单词 W_t=“Bert” 前面句子的t-1个单词，要求网络正确预测单词Bert，即是的如下公式的P值最大化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(W_t=&quot;Bert&quot;|W_1,W_2,…W_(t-1);θ)</span><br></pre></td></tr></table></figure><p>其中 W_t 前面的词语 W_i 使用Onehot进行编码，作为单词的输出，然后乘以矩阵Q后获得向量 C(W_i)，每个单词的C进行拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。 C(W_i)就是单词对应的Word embedding值。</p><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Word2Vec是一个Word Embedding的工具，除此之外，Glove也是一个有名的工具，这里主要介绍Word2Vec的工作原理。</p><p>Word2Vec的网络结构和NNLM是基本类似的，而且也是做语言模型任务，但是其训练方法不太一样。</p><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/3.png" alt="image"></p><p>Word2Vec有两种训练方法：</p><ul><li>CBOW：核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词。</li><li>Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。（输入一个词语，在不同的上下文环境下求条件概率，取得概率最高的作为最佳结果。）</li></ul><h2 id="如何使用WE"><a href="#如何使用WE" class="headerlink" title="如何使用WE"></a>如何使用WE</h2><p>与图像领域类似，有两种使用方式：</p><ul><li>Frozen：就是Word Embedding那层网络参数固定不动</li><li>Fine-Tuning：就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新</li></ul><h2 id="WE存在的问题"><a href="#WE存在的问题" class="headerlink" title="WE存在的问题"></a>WE存在的问题</h2><p>使用WE的效果并没有预想的好，主要问题是在NLP领域的多义词问题。比如：Bank的意思有银行和河岸两种，但是其WE表示只有一种无法进行区分。ELMO提供了一种简洁优雅的解决方案。</p><h1 id="Word-Embedding到ELMO"><a href="#Word-Embedding到ELMO" class="headerlink" title="Word Embedding到ELMO"></a>Word Embedding到ELMO</h1><h2 id="原理及使用"><a href="#原理及使用" class="headerlink" title="原理及使用"></a>原理及使用</h2><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/4.png" alt="image"></p><p>第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。新特征指的是，根据上下文单词的语义去调整得到的单词的Word Embedding表示。</p><p>ELMO的到了三个embedding：单词特征（word embedding），句法特征，语义特征。这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><ul><li>优点：解决了多义词问题，同时，还能获取词语的词性特征，在句子语义关系判断，分类任务，阅读理解等多个领域表现很好，适用范围广、普适性强。</li><li>缺点：<ul><li>LSTM特征抽取能力若于Transformer</li><li>双向拼接融合特征的能力弱于Bert的一体化融合能力</li></ul></li></ul><p><strong><em>总的来说，ELMO是通过LSTM获取了更多维度的信息，并将多维度的特征进行加权融合。</em></strong></p><h1 id="Word-Embedding到GPT"><a href="#Word-Embedding到GPT" class="headerlink" title="Word Embedding到GPT"></a>Word Embedding到GPT</h1><h2 id="原理及使用-1"><a href="#原理及使用-1" class="headerlink" title="原理及使用"></a>原理及使用</h2><p>GPT的特征提取方式采用了fine-tuning。“Generative Pre-Training”的简称，生成式的预训练。</p><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/5.png" alt="image"></p><p>第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。与ELMO的区别主要在于：1） 特征抽取器不是用的RNN，而是用的Transformer；2）GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型（这是GPT的一个缺点）。</p><p><strong><em>注：Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器</em></strong></p><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/6.png" alt="image"></p><p>使用时，利用第一步预训练好的参数初始化GPT的网络结构，然后根据新任务训练网络进行Fine-tuning，使得这个网络更适合解决新任务。</p><h2 id="下游任务改造"><a href="#下游任务改造" class="headerlink" title="下游任务改造"></a>下游任务改造</h2><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/7.png" alt="image"></p><p>从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可：</p><ul><li>对于分类问题，加上一个起始和终结符号即可</li><li>对于句子关系判断问题，两个句子中间再加个分隔符即可</li><li>对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要</li><li>对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可</li></ul><h1 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h1><h2 id="原理及使用-2"><a href="#原理及使用-2" class="headerlink" title="原理及使用"></a>原理及使用</h2><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/8.png" alt="image"></p><p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的 <strong><em>双向语言模型</em></strong> ，当然另外一点是语言模型 <strong><em>数据规模要比GPT大</em></strong> 。其使用方式也与GPT类似。</p><h2 id="下游任务改造-1"><a href="#下游任务改造-1" class="headerlink" title="下游任务改造"></a>下游任务改造</h2><p>主要任务包括：</p><ul><li>序列列标注：分词/POS Tag/NER/语义标注…</li><li>分类任务：文本分类／情感计算…</li><li>句子关系判断：Entailment/QA/自然语言推理…</li><li>生成式任务：机器翻译／⽂本摘要…</li></ul><p>不同任务改造方式，除了对输入进行改造，输出也需要改造。经过对S2S改造，bert也可用到机器翻译或者文本摘要，聊天机器人这种生成式任务。</p><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/9.png" alt="image"></p><p>从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><img src="//blog.xixilili.cn/2019/05/24/自然语言处理中的预训练-总结/10.png" alt="image"></p><p><strong><em>本文参考了知乎专栏文章，对其内容进行了个性化总结，补充了自己的理解部分，有关bert部分会单独整理学习笔记</em></strong></p><p>参考资料</p><ul><li><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 预训练 </tag>
            
            <tag> 综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>百度文库爬虫</title>
      <link href="/2019/05/23/%E7%99%BE%E5%BA%A6%E6%96%87%E5%BA%93%E7%88%AC%E8%99%AB/"/>
      <url>/2019/05/23/%E7%99%BE%E5%BA%A6%E6%96%87%E5%BA%93%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="FreeForWenku"><a href="#FreeForWenku" class="headerlink" title="FreeForWenku"></a>FreeForWenku</h1><p>免费下载百度文库收费资料，支持关键字搜索，以及url批量爬取。目前本项目仅对原项目的doc、txt爬取优化，其余的待完成优化。</p><p>项目地址：<a href="https://github.com/whxf/FreeForWenku" target="_blank" rel="noopener">whxf/FreeForWenku</a></p><p><strong><em>声明：</em></strong> 本项目fork 自 <a href="https://github.com/Lz1y/FreeForWenku" target="_blank" rel="noopener">Lz1y/FreeForWenku</a>。对原有项目进行优化，添加支持关键字搜索，url批量爬取，logger等功能。</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><ul><li><strong><em>Step 1：</em></strong> Clone项目，安装依赖(本项目基于Python3.6开发)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><ul><li><strong><em>Step 2：</em></strong> 修改main.py中的参数 </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要修改的参数</span></span><br><span class="line">search_word = <span class="string">'一站到底'</span>  <span class="comment">#  待搜索关键词</span></span><br><span class="line">max_page = <span class="number">1</span>  <span class="comment"># 最大的检索页数</span></span><br><span class="line"><span class="comment"># 无需求修改的参数</span></span><br><span class="line">session = requests.session()</span><br><span class="line">url_output_dir = os.path.join(os.getcwd(), <span class="string">'url_outputs'</span>)  <span class="comment"># 关键词搜索url结果输出位置</span></span><br><span class="line">txt_output_dir = os.path.join(os.getcwd(), <span class="string">'txt_outputs'</span>)  <span class="comment"># 根据url获取txt文件的输出位置</span></span><br></pre></td></tr></table></figure><ul><li><strong><em>Step 3：</em></strong>  run main.py</li></ul><h2 id="项目详细介绍"><a href="#项目详细介绍" class="headerlink" title="项目详细介绍"></a>项目详细介绍</h2><h3 id="爬取思路"><a href="#爬取思路" class="headerlink" title="爬取思路"></a>爬取思路</h3><ol><li>搜索关键词，获取百度文库查询结果（文件url和type）<ul><li>对关键词进行GB2312编码，整合request url</li><li>文件存储在url_outputs中，存储为json格式，文件以关键词命名</li></ul></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;type&quot;: &quot;doc&quot;,</span><br><span class="line">&quot;url&quot;: &quot;https://wenku.baidu.com/view/b6d9fdb5b4daa58da1114a91.html?from=search&quot;</span><br><span class="line">&#125;,</span><br><span class="line">    ... ...</span><br><span class="line">]</span><br></pre></td></tr></table></figure><ol start="2"><li>根据url和type爬取文件类容<ul><li>核心的爬取思路是：通过百度文库的复制接口获取文字片段，将文字片段根据行号进行拼接</li><li>DOC和TXT文件使用不同的爬取方法</li><li>爬取的结果存储在txt_outputs/search_word中</li></ul></li></ol><h3 id="文件目录"><a href="#文件目录" class="headerlink" title="文件目录"></a>文件目录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">FreeFromWenku</span><br><span class="line">│  main.py              程序入口</span><br><span class="line">│  README.md            read me</span><br><span class="line">│  requirements.txt     依赖项</span><br><span class="line">│</span><br><span class="line">├─log</span><br><span class="line">│      error.log        error log</span><br><span class="line">│</span><br><span class="line">├─txt_outputs           根据url获取txt文件的输出位置</span><br><span class="line">│  └─一站到底           每个关键词的相关文件存储在一个文件夹中</span><br><span class="line">│          “一站到底”活动方案.txt</span><br><span class="line">│</span><br><span class="line">├─url_outputs               关键词搜索url结果输出位置</span><br><span class="line">│      一站到底.json        关键词搜索获取的url</span><br><span class="line">│</span><br><span class="line">└─utils                     可复用的模块</span><br><span class="line">      logger.py             logger      </span><br><span class="line">      url_to_txt.py         包含根据url获取doc和txt文件</span><br><span class="line">      word_to_url.py        搜索关键词，获取结果url</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 百度文库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/05/22/hello-world/"/>
      <url>/2019/05/22/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
