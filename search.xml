<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[自然语言处理中的预训练]]></title>
    <url>%2F2019%2F05%2F24%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[“预训练”的思路来源于图像领域，因此，首先追本溯源，从图像角度出发了解其基本思路。 图像领域的预训练什么是预训练以简要的预训练步骤进行说明： 设计好网络结构 在任务A上训练模型、学习参数 保存模型参数，以后备用（即与训练模型） 如何使用与训练模型使用与训练模型一般有两种做法： Frozen：浅层加载的参数在新的任务上不改变。（一个简单的例子，加载预训练词向量，在模型训练中，词向量参数不改变） Fine-Tuning：把参数进行微调，使得更适应当前任务。（同样以词向量为例，加载预训练词向量，词向量不固定，可随模型的训练改变） 使用预训练的优势问题： 新任务训练数据少，则不能很好的训练复杂网络。 解决方案： 此时可使用大的数据集预训练模型，然后根据新任务的数据进行Fine-Tuning，使其能够解决新任务，如：ImageNet的使用。 为什么预训练的思路可行预训练好的网络参数，底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性。因此，一般用底层预训练好的参数初始化新任务网络参数。与之相反，高层特征跟任务关联较大。 在图像领域，越是底层的特征越是不论什么领域的图像都会具备的，比如：边角线弧线等底层基础特征。在高层则可学习到任务相关的特征，如：五官的轮廓、脸轮廓等特征。 Word EmbeddingWord Embedding是从语言模型一步步演化形成的，下面魔术Word Embedding是如何形成的。 语言模型（LM）评估哪个句子更像合理的句子，根据句子中词语的上下文信息，计算条件概率进行评估。 神经网络语言模型（NNLM） 学习任务是输入某个句中单词 W_t=“Bert” 前面句子的t-1个单词，要求网络正确预测单词Bert，即是的如下公式的P值最大化： 1P(W_t=&quot;Bert&quot;|W_1,W_2,…W_(t-1);θ) 其中 W_t 前面的词语 W_i 使用Onehot进行编码，作为单词的输出，然后乘以矩阵Q后获得向量 C(W_i)，每个单词的C进行拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。 C(W_i)就是单词对应的Word embedding值。 Word2VecWord2Vec是一个Word Embedding的工具，除此之外，Glove也是一个有名的工具，这里主要介绍Word2Vec的工作原理。 Word2Vec的网络结构和NNLM是基本类似的，而且也是做语言模型任务，但是其训练方法不太一样。 Word2Vec有两种训练方法： CBOW：核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词。 Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。（输入一个词语，在不同的上下文环境下求条件概率，取得概率最高的作为最佳结果。） 如何使用WE与图像领域类似，有两种使用方式： Frozen：就是Word Embedding那层网络参数固定不动 Fine-Tuning：就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新 WE存在的问题使用WE的效果并没有预想的好，主要问题是在NLP领域的多义词问题。比如：Bank的意思有银行和河岸两种，但是其WE表示只有一种无法进行区分。ELMO提供了一种简洁优雅的解决方案。 Word Embedding到ELMO原理及使用 第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。新特征指的是，根据上下文单词的语义去调整得到的单词的Word Embedding表示。 ELMO的到了三个embedding：单词特征（word embedding），句法特征，语义特征。这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。 优缺点 优点：解决了多义词问题，同时，还能获取词语的词性特征，在句子语义关系判断，分类任务，阅读理解等多个领域表现很好，适用范围广、普适性强。 缺点： LSTM特征抽取能力若于Transformer 双向拼接融合特征的能力弱于Bert的一体化融合能力 总的来说，ELMO是通过LSTM获取了更多维度的信息，并将多维度的特征进行加权融合。 Word Embedding到GPT原理及使用GPT的特征提取方式采用了fine-tuning。“Generative Pre-Training”的简称，生成式的预训练。 第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。与ELMO的区别主要在于：1） 特征抽取器不是用的RNN，而是用的Transformer；2）GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型（这是GPT的一个缺点）。 注：Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器 使用时，利用第一步预训练好的参数初始化GPT的网络结构，然后根据新任务训练网络进行Fine-tuning，使得这个网络更适合解决新任务。 下游任务改造 从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可： 对于分类问题，加上一个起始和终结符号即可 对于句子关系判断问题，两个句子中间再加个分隔符即可 对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要 对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可 Bert原理及使用 Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的 双向语言模型 ，当然另外一点是语言模型 数据规模要比GPT大 。其使用方式也与GPT类似。 下游任务改造主要任务包括： 序列列标注：分词/POS Tag/NER/语义标注… 分类任务：文本分类／情感计算… 句子关系判断：Entailment/QA/自然语言推理… 生成式任务：机器翻译／⽂本摘要… 不同任务改造方式，除了对输入进行改造，输出也需要改造。经过对S2S改造，bert也可用到机器翻译或者文本摘要，聊天机器人这种生成式任务。 从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。 总结 本文参考了知乎专栏文章，对其内容进行了个性化总结，补充了自己的理解部分，有关bert部分会单独整理学习笔记 参考资料 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>预训练</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度文库爬虫]]></title>
    <url>%2F2019%2F05%2F23%2F%E7%99%BE%E5%BA%A6%E6%96%87%E5%BA%93%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[FreeForWenku免费下载百度文库收费资料，支持关键字搜索，以及url批量爬取。目前本项目仅对原项目的doc、txt爬取优化，其余的待完成优化。 项目地址：whxf/FreeForWenku 声明： 本项目fork 自 Lz1y/FreeForWenku。对原有项目进行优化，添加支持关键字搜索，url批量爬取，logger等功能。 使用方法 Step 1： Clone项目，安装依赖(本项目基于Python3.6开发) 1pip install -r requirements.txt Step 2： 修改main.py中的参数 1234567# 需要修改的参数search_word = '一站到底' # 待搜索关键词max_page = 1 # 最大的检索页数# 无需求修改的参数session = requests.session()url_output_dir = os.path.join(os.getcwd(), 'url_outputs') # 关键词搜索url结果输出位置txt_output_dir = os.path.join(os.getcwd(), 'txt_outputs') # 根据url获取txt文件的输出位置 Step 3： run main.py 项目详细介绍爬取思路 搜索关键词，获取百度文库查询结果（文件url和type） 对关键词进行GB2312编码，整合request url 文件存储在url_outputs中，存储为json格式，文件以关键词命名 1234567[ &#123; &quot;type&quot;: &quot;doc&quot;, &quot;url&quot;: &quot;https://wenku.baidu.com/view/b6d9fdb5b4daa58da1114a91.html?from=search&quot; &#125;, ... ...] 根据url和type爬取文件类容 核心的爬取思路是：通过百度文库的复制接口获取文字片段，将文字片段根据行号进行拼接 DOC和TXT文件使用不同的爬取方法 爬取的结果存储在txt_outputs/search_word中 文件目录12345678910111213141516171819FreeFromWenku│ main.py 程序入口│ README.md read me│ requirements.txt 依赖项│├─log│ error.log error log│├─txt_outputs 根据url获取txt文件的输出位置│ └─一站到底 每个关键词的相关文件存储在一个文件夹中│ “一站到底”活动方案.txt│├─url_outputs 关键词搜索url结果输出位置│ 一站到底.json 关键词搜索获取的url│└─utils 可复用的模块 logger.py logger url_to_txt.py 包含根据url获取doc和txt文件 word_to_url.py 搜索关键词，获取结果url]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>百度文库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
