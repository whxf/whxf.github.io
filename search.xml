<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[「BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding」总结]]></title>
    <url>%2F2019%2F05%2F29%2F%E3%80%8CBERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding%E3%80%8D%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[BERT Model Architecturebert model是多层双向transformer编码器。详细可参考Attention is all you need。 这里需要了解bert模型的相关参数： L: number of layer, e.g. transformer block H: hiddensize A: self-attention head number bert模型有两个大小： base：L=12，H=768，A=12，total parameters=340M large：L=24，H=1024，A=16，total parameters=110M Input Presentation 输入是形式sentence pair，e.g. [question, answer]的pair，[A, B]。输入的表示由三部分构成（求和）： segment embedding：句子embedding，因为前面提到训练数据都是由两个句子构成的，那么 每个句子有个句子整体的embedding项对应给每个单词 token embedding：单词embedding,这个就是我们之前一直提到的 单词embedding position embedding：位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对 位置信息进行编码 有几个注意事项： word pieces：数据中的有些单词会被分割为word piece，playing这类词在数据中会被表示为play和##ing，其中##是word piece的分割标记。 sequence length：sentence pair的总长度小于等于512 tokens 特殊标志：[CLS]表示句子的开始, [SEP]表示句子的分割 在预训练的时候，若使用单个句子则仅使用[A, B]中的A Pre-training TaskMasked LM任务简述：将句子中的一些词mask掉，然后预测mask的词语是什么。 在这个任务中，对mask进行了特殊处理： 对一个sequence中15%的word piece token进行随机mask 使用[mask]替换原来的word，但并非所有的都用[mask]替换，替换规则是： 80%的mask使用[mask]标志进行替换 10%的mask用随机的其他token替代 10%的mask保持使用原本的token 12Input: the man went to the [MASK1] . he bought a [MASK2] of milk.Labels: [MASK1] = store; [MASK2] = gallon 这样做的意义是：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。 Next Sentence Prediction任务简述：对于[A, B]预测sentence B是不是sentence A的下一个句子，如果是label是 IsNextSentence， 如果不是标注为 NotNextSentence，负例的构造方式是，保留A然后在预料中随机找另一个句子作为B，正负例的比例是1:1 123Sentence A: the man went to the store .Sentence B: he bought a gallon of milk .Label: IsNextSentence 123Sentence A: the man went to the store .Sentence B: penguins are flightless .Label: NotNextSentence 这样做的意义是：我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。 Pre-training Procedure介绍了dataset，generate input sequence，task的运行，参数的设置等 Fine-tuning Procedure介绍了对分类问题的fine-tuning 查缺补漏Position EmbeddingPosition Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。 Position Embedding的公式如下所示，这里的意思是将为p的位置映射为一个 d_pos 维的位置向量，这个向量的第i个元素的数值就是 PE_i(p)。 123PE_&#123;2i&#125;(p) = sin(p/ 10000^&#123;&#123;2i&#125; / &#123;d_&#123;pos&#125;&#125;&#125; )PE_&#123;2i+1&#125;(p)=cos(p/10000^&#123;&#123;2i&#125;/d_&#123;pos&#125;&#125;) Position Embedding本身是一个绝对位置的信息，但在语言中，相对位置也很重要，Google选择前述的位置向量公式的一个重要原因是：由于我们有sin(α+β)=sinαcosβ+cosαsinβ以及cos(α+β)=cosαcosβ−sinαsinβ，这表明位置p+k的向量可以表示成位置p的向量的线性变换，这提供了表达相对位置信息的可能性。 结合位置向量和词向量有几个可选方案，可以把它们拼接起来作为一个新向量，也可以把位置向量定义为跟词向量一样大小，然后两者加起来。 参考资料： 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史 google-research/bert BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 《Attention is All You Need》浅读（简介+代码）]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>bert</tag>
        <tag>论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言处理中的预训练]]></title>
    <url>%2F2019%2F05%2F24%2F%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83-%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[“预训练”的思路来源于图像领域，因此，首先追本溯源，从图像角度出发了解其基本思路。 图像领域的预训练什么是预训练以简要的预训练步骤进行说明： 设计好网络结构 在任务A上训练模型、学习参数 保存模型参数，以后备用（即与训练模型） 如何使用与训练模型使用与训练模型一般有两种做法： Frozen：浅层加载的参数在新的任务上不改变。（一个简单的例子，加载预训练词向量，在模型训练中，词向量参数不改变） Fine-Tuning：把参数进行微调，使得更适应当前任务。（同样以词向量为例，加载预训练词向量，词向量不固定，可随模型的训练改变） 使用预训练的优势问题： 新任务训练数据少，则不能很好的训练复杂网络。 解决方案： 此时可使用大的数据集预训练模型，然后根据新任务的数据进行Fine-Tuning，使其能够解决新任务，如：ImageNet的使用。 为什么预训练的思路可行预训练好的网络参数，底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性。因此，一般用底层预训练好的参数初始化新任务网络参数。与之相反，高层特征跟任务关联较大。 在图像领域，越是底层的特征越是不论什么领域的图像都会具备的，比如：边角线弧线等底层基础特征。在高层则可学习到任务相关的特征，如：五官的轮廓、脸轮廓等特征。 Word EmbeddingWord Embedding是从语言模型一步步演化形成的，下面魔术Word Embedding是如何形成的。 语言模型（LM）评估哪个句子更像合理的句子，根据句子中词语的上下文信息，计算条件概率进行评估。 神经网络语言模型（NNLM） 学习任务是输入某个句中单词 W_t=“Bert” 前面句子的t-1个单词，要求网络正确预测单词Bert，即是的如下公式的P值最大化： 1P(W_t=&quot;Bert&quot;|W_1,W_2,…W_(t-1);θ) 其中 W_t 前面的词语 W_i 使用Onehot进行编码，作为单词的输出，然后乘以矩阵Q后获得向量 C(W_i)，每个单词的C进行拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。 C(W_i)就是单词对应的Word embedding值。 Word2VecWord2Vec是一个Word Embedding的工具，除此之外，Glove也是一个有名的工具，这里主要介绍Word2Vec的工作原理。 Word2Vec的网络结构和NNLM是基本类似的，而且也是做语言模型任务，但是其训练方法不太一样。 Word2Vec有两种训练方法： CBOW：核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词。 Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。（输入一个词语，在不同的上下文环境下求条件概率，取得概率最高的作为最佳结果。） 如何使用WE与图像领域类似，有两种使用方式： Frozen：就是Word Embedding那层网络参数固定不动 Fine-Tuning：就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新 WE存在的问题使用WE的效果并没有预想的好，主要问题是在NLP领域的多义词问题。比如：Bank的意思有银行和河岸两种，但是其WE表示只有一种无法进行区分。ELMO提供了一种简洁优雅的解决方案。 Word Embedding到ELMO原理及使用 第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。新特征指的是，根据上下文单词的语义去调整得到的单词的Word Embedding表示。 ELMO的到了三个embedding：单词特征（word embedding），句法特征，语义特征。这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。 优缺点 优点：解决了多义词问题，同时，还能获取词语的词性特征，在句子语义关系判断，分类任务，阅读理解等多个领域表现很好，适用范围广、普适性强。 缺点： LSTM特征抽取能力若于Transformer 双向拼接融合特征的能力弱于Bert的一体化融合能力 总的来说，ELMO是通过LSTM获取了更多维度的信息，并将多维度的特征进行加权融合。 Word Embedding到GPT原理及使用GPT的特征提取方式采用了fine-tuning。“Generative Pre-Training”的简称，生成式的预训练。 第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。与ELMO的区别主要在于：1） 特征抽取器不是用的RNN，而是用的Transformer；2）GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型（这是GPT的一个缺点）。 注：Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器 使用时，利用第一步预训练好的参数初始化GPT的网络结构，然后根据新任务训练网络进行Fine-tuning，使得这个网络更适合解决新任务。 下游任务改造 从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可： 对于分类问题，加上一个起始和终结符号即可 对于句子关系判断问题，两个句子中间再加个分隔符即可 对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要 对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可 Bert原理及使用 Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的 双向语言模型 ，当然另外一点是语言模型 数据规模要比GPT大 。其使用方式也与GPT类似。 下游任务改造主要任务包括： 序列列标注：分词/POS Tag/NER/语义标注… 分类任务：文本分类／情感计算… 句子关系判断：Entailment/QA/自然语言推理… 生成式任务：机器翻译／⽂本摘要… 不同任务改造方式，除了对输入进行改造，输出也需要改造。经过对S2S改造，bert也可用到机器翻译或者文本摘要，聊天机器人这种生成式任务。 从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。 总结 本文参考了知乎专栏文章，对其内容进行了个性化总结，补充了自己的理解部分，有关bert部分会单独整理学习笔记 参考资料 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>预训练</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度文库爬虫]]></title>
    <url>%2F2019%2F05%2F23%2F%E7%99%BE%E5%BA%A6%E6%96%87%E5%BA%93%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[FreeForWenku免费下载百度文库收费资料，支持关键字搜索，以及url批量爬取。目前本项目仅对原项目的doc、txt爬取优化，其余的待完成优化。 项目地址：whxf/FreeForWenku 声明： 本项目fork 自 Lz1y/FreeForWenku。对原有项目进行优化，添加支持关键字搜索，url批量爬取，logger等功能。 使用方法 Step 1： Clone项目，安装依赖(本项目基于Python3.6开发) 1pip install -r requirements.txt Step 2： 修改main.py中的参数 1234567# 需要修改的参数search_word = '一站到底' # 待搜索关键词max_page = 1 # 最大的检索页数# 无需求修改的参数session = requests.session()url_output_dir = os.path.join(os.getcwd(), 'url_outputs') # 关键词搜索url结果输出位置txt_output_dir = os.path.join(os.getcwd(), 'txt_outputs') # 根据url获取txt文件的输出位置 Step 3： run main.py 项目详细介绍爬取思路 搜索关键词，获取百度文库查询结果（文件url和type） 对关键词进行GB2312编码，整合request url 文件存储在url_outputs中，存储为json格式，文件以关键词命名 1234567[ &#123; &quot;type&quot;: &quot;doc&quot;, &quot;url&quot;: &quot;https://wenku.baidu.com/view/b6d9fdb5b4daa58da1114a91.html?from=search&quot; &#125;, ... ...] 根据url和type爬取文件类容 核心的爬取思路是：通过百度文库的复制接口获取文字片段，将文字片段根据行号进行拼接 DOC和TXT文件使用不同的爬取方法 爬取的结果存储在txt_outputs/search_word中 文件目录12345678910111213141516171819FreeFromWenku│ main.py 程序入口│ README.md read me│ requirements.txt 依赖项│├─log│ error.log error log│├─txt_outputs 根据url获取txt文件的输出位置│ └─一站到底 每个关键词的相关文件存储在一个文件夹中│ “一站到底”活动方案.txt│├─url_outputs 关键词搜索url结果输出位置│ 一站到底.json 关键词搜索获取的url│└─utils 可复用的模块 logger.py logger url_to_txt.py 包含根据url获取doc和txt文件 word_to_url.py 搜索关键词，获取结果url]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>百度文库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
